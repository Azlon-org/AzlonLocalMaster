[
    {
        "role": "user",
        "content": "You are a Python coding assistant. Your task is to generate Python code for the following data analysis task.\nEnsure your code is self-contained, uses common libraries like pandas and numpy, and is ready to run.\nThe code will be executed within a function wrapper, so avoid top-level `if __name__ == '__main__':` blocks in your direct output.\nPrioritize generating actionable business insights, such as identifying trends, patterns, correlations, or anomalies in the data. While basic data cleaning might be necessary if data quality issues impede analysis, the primary goal is to uncover valuable information that can inform business decisions. If the 'Analysis Plan Task to Implement' is specifically about data cleaning or preparation, focus on that. Otherwise, aim for insightful analysis.\nProduce outputs that clearly communicate these insights. This includes printing statistical summaries and key observations to standard output, and generating informative visualizations (e.g., bar charts, line plots, scatter plots, heatmaps as appropriate for the data and task). Save all generated plots to files in the `data_directory_path` (which will be provided as a variable) and print a confirmation message to standard output stating the filename of the saved plot.\n\n**Business Context:**\nQuickWash operates a platform (app and website) that connects car detailers (\u201coperators\u201d) directly with customers. Customers request car wash services through the app, and detailers travel to the customer\u2019s location to perform the wash. The business model revolves around on-demand mobile car washing and detailing services with various service packages and potential extras.\n\n**Data Summary/Available Data:**\nNo data summary found.\n\n**Data Directory Path:**\nmulti_agents/competition/CarWash_Data\n\n**Analysis Plan Task to Implement:**\n**Data Quality and Integrity Validation**\n\nWhen loading data files (e.g., CSVs), assume a variable `data_directory_path` is predefined in the execution scope and contains the 'Data Directory Path' shown above. Construct full paths to data files using this variable and `os.path.join()`. For example, to load `orders.csv`, use `pd.read_csv(os.path.join(data_directory_path, 'orders.csv'))`.\nPlease include basic error handling, such as try-except blocks around file I/O operations and critical data processing steps.\nPlease provide ONLY the Python code block for this task, without any surrounding explanations unless they are comments within the code."
    },
    {
        "role": "assistant",
        "content": "```python\nimport os\nimport pandas as pd\nimport numpy as np\n\ndef analyze_data_quality_and_integrity(data_directory_path):\n    files_to_check = ['orders.csv', 'operators.csv', 'customers.csv', 'services.csv', 'transactions.csv']\n    dataframes = {}\n\n    # Load datasets with error handling\n    for file in files_to_check:\n        file_path = os.path.join(data_directory_path, file)\n        try:\n            df = pd.read_csv(file_path)\n            dataframes[file] = df\n            print(f\"Loaded '{file}' successfully: {df.shape[0]} rows, {df.shape[1]} columns.\")\n        except FileNotFoundError:\n            print(f\"ERROR: File '{file}' not found in directory '{data_directory_path}'. Skipping.\")\n        except pd.errors.ParserError:\n            print(f\"ERROR: Parsing error encountered while reading '{file}'. Skipping.\")\n        except Exception as e:\n            print(f\"ERROR: Unexpected error loading '{file}': {e}\")\n\n    # If no data loaded, exit\n    if not dataframes:\n        print(\"No data files loaded. Exiting data quality validation.\")\n        return\n\n    # Function to check nulls and duplicates and print summary\n    def check_df_quality(df, df_name):\n        print(f\"\\nData Quality Report for '{df_name}':\")\n        # Basic info\n        print(f\" - Shape: {df.shape}\")\n        print(f\" - Columns: {list(df.columns)}\")\n\n        # Null values\n        null_counts = df.isnull().sum()\n        null_percent = (null_counts / len(df)) * 100\n        null_summary = pd.DataFrame({'null_count': null_counts, 'null_percent': null_percent})\n        print(\" - Null values per column:\")\n        print(null_summary[null_summary['null_count'] > 0])\n\n        # Duplicate rows\n        dup_count = df.duplicated().sum()\n        print(f\" - Duplicate rows: {dup_count}\")\n\n        # Unique values per column (for categorical or ID columns)\n        unique_counts = df.nunique(dropna=False)\n        print(\" - Unique values per column:\")\n        print(unique_counts)\n\n        # Data types\n        print(\" - Data types:\")\n        print(df.dtypes)\n\n    # Run quality checks on each loaded dataframe\n    for file, df in dataframes.items():\n        check_df_quality(df, file)\n\n    # Cross-file basic integrity checks if relevant files exist\n    print(\"\\nCross-file Integrity Checks:\")\n\n    # Example: Check if all operator_ids in orders exist in operators\n    if 'orders.csv' in dataframes and 'operators.csv' in dataframes:\n        orders = dataframes['orders.csv']\n        operators = dataframes['operators.csv']\n        missing_ops = set(orders['operator_id'].dropna().unique()) - set(operators['operator_id'].dropna().unique())\n        print(f\" - Operators referenced in orders but missing in operators.csv: {len(missing_ops)}\")\n        if len(missing_ops) > 0:\n            print(f\"   Missing operator_ids sample: {list(missing_ops)[:5]}\")\n\n    # Check if all customer_ids in orders exist in customers\n    if 'orders.csv' in dataframes and 'customers.csv' in dataframes:\n        customers = dataframes['customers.csv']\n        missing_custs = set(orders['customer_id'].dropna().unique()) - set(customers['customer_id'].dropna().unique())\n        print(f\" - Customers referenced in orders but missing in customers.csv: {len(missing_custs)}\")\n        if len(missing_custs) > 0:\n            print(f\"   Missing customer_ids sample: {list(missing_custs)[:5]}\")\n\n    # Check for date/time inconsistencies in orders (e.g., order_date before today, or invalid dates)\n    if 'orders.csv' in dataframes:\n        try:\n            orders = dataframes['orders.csv']\n            if 'order_date' in orders.columns:\n                orders['order_date_parsed'] = pd.to_datetime(orders['order_date'], errors='coerce')\n                invalid_dates = orders['order_date_parsed'].isna().sum()\n                print(f\" - Orders with invalid or missing 'order_date': {invalid_dates}\")\n\n                future_dates = (orders['order_date_parsed'] > pd.Timestamp.now()).sum()\n                print(f\" - Orders with 'order_date' in the future: {future_dates}\")\n        except Exception as e:\n            print(f\"ERROR: Failed datetime validation on orders: {e}\")\n\n    # Check for negative or zero values in numeric columns where not logical\n    for file, df in dataframes.items():\n        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n        if numeric_cols:\n            for col in numeric_cols:\n                neg_vals = (df[col] < 0).sum()\n                zeros = (df[col] == 0).sum()\n                if neg_vals > 0:\n                    print(f\" - Negative values found in '{col}' column of '{file}': {neg_vals}\")\n                # For zero values, only flag if column likely should not have zeros (e.g., price, duration)\n                if zeros > 0 and any(x in col.lower() for x in ['price','amount','duration','cost']):\n                    print(f\" - Zero values found in '{col}' column of '{file}': {zeros}\")\n\n    print(\"\\nData Quality and Integrity Validation Completed.\")\n```"
    }
]