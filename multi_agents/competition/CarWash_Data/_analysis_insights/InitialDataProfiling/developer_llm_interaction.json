[
    {
        "role": "user",
        "content": "You are a Python coding assistant. Your task is to generate Python code for the following data analysis task.\nEnsure your code is self-contained, uses common libraries like pandas and numpy, and is ready to run.\nThe code will be executed within a function wrapper, so avoid top-level `if __name__ == '__main__':` blocks in your direct output.\nPrioritize generating actionable business insights, such as identifying trends, patterns, correlations, or anomalies in the data. While basic data cleaning might be necessary if data quality issues impede analysis, the primary goal is to uncover valuable information that can inform business decisions. If the 'Analysis Plan Task to Implement' is specifically about data cleaning or preparation, focus on that. Otherwise, aim for insightful analysis.\nProduce outputs that clearly communicate these insights. This includes printing statistical summaries and key observations to standard output, and generating informative visualizations (e.g., bar charts, line plots, scatter plots, heatmaps as appropriate for the data and task). Save all generated plots to files in the `data_directory_path` (which will be provided as a variable) and print a confirmation message to standard output stating the filename of the saved plot.\n\n**Business Context:**\nQuickWash operates a platform (app and website) that connects car detailers (\u201coperators\u201d) directly with customers. Customers request car wash services through the app, and detailers travel to the customer\u2019s location to perform the wash. The business model revolves around on-demand mobile car washing and detailing services with various service packages and potential extras.\n\n**Data Summary/Available Data:**\nNo data summary found.\n\n**Data Directory Path:**\nmulti_agents/competition/CarWash_Data\n\n**Analysis Plan Task to Implement:**\nRepair and validate corrupted critical datasets (`clients.csv`, `orders.csv`, `operators.csv`, `operator_balance.csv`, `activity_logs.csv`) to enable reliable analysis.\n\nWhen loading data files (e.g., CSVs), assume a variable `data_directory_path` is predefined in the execution scope and contains the 'Data Directory Path' shown above. Construct full paths to data files using this variable and `os.path.join()`. For example, to load `orders.csv`, use `pd.read_csv(os.path.join(data_directory_path, 'orders.csv'))`.\nPlease include basic error handling, such as try-except blocks around file I/O operations and critical data processing steps.\nPlease provide ONLY the Python code block for this task, without any surrounding explanations unless they are comments within the code."
    },
    {
        "role": "assistant",
        "content": "```python\nimport os\nimport pandas as pd\nimport numpy as np\n\ndef repair_and_validate_datasets(data_directory_path):\n    # Helper function to load CSV with error handling\n    def load_csv(file_name):\n        path = os.path.join(data_directory_path, file_name)\n        try:\n            df = pd.read_csv(path)\n            print(f\"Loaded {file_name} with shape {df.shape}\")\n            return df\n        except FileNotFoundError:\n            print(f\"Error: {file_name} not found in {data_directory_path}\")\n            return None\n        except pd.errors.ParserError:\n            print(f\"Error: Parsing {file_name} failed - possible corruption or malformed CSV.\")\n            return None\n        except Exception as e:\n            print(f\"Error loading {file_name}: {e}\")\n            return None\n\n    # Helper function to detect and report missing values and duplicates\n    def data_overview(df, name):\n        if df is None:\n            print(f\"Skipping overview for {name} - dataframe not loaded.\")\n            return\n        print(f\"\\n--- Data Overview: {name} ---\")\n        print(f\"Columns: {df.columns.tolist()}\")\n        print(f\"Shape: {df.shape}\")\n        missing = df.isnull().sum()\n        if missing.sum() > 0:\n            print(f\"Missing values per column:\\n{missing[missing > 0]}\")\n        else:\n            print(\"No missing values detected.\")\n        dup_count = df.duplicated().sum()\n        print(f\"Duplicate rows: {dup_count}\")\n\n    # Helper function to attempt basic repairs\n    def repair_df(df, name):\n        if df is None:\n            return None\n\n        # Remove fully duplicate rows\n        before = df.shape[0]\n        df = df.drop_duplicates()\n        after = df.shape[0]\n        if before != after:\n            print(f\"{name}: Removed {before - after} duplicate rows.\")\n\n        # For critical columns, we try to fix types and fill missing if possible\n        # We'll define some common fixes per dataset below\n\n        # Repair based on dataset name\n        if name == 'clients.csv':\n            # Expect client_id as primary key - drop rows missing it\n            if 'client_id' in df.columns:\n                before = df.shape[0]\n                df = df.dropna(subset=['client_id'])\n                after = df.shape[0]\n                if before != after:\n                    print(f\"{name}: Dropped {before - after} rows with missing client_id.\")\n                # Try to convert client_id to int if not already\n                df['client_id'] = pd.to_numeric(df['client_id'], errors='coerce')\n                before = df.shape[0]\n                df = df.dropna(subset=['client_id'])\n                df['client_id'] = df['client_id'].astype(int)\n                after = df.shape[0]\n                if before != after:\n                    print(f\"{name}: Dropped {before - after} rows with non-numeric client_id.\")\n            # Fill missing categorical info with 'Unknown' if any\n            for col in df.select_dtypes(include='object').columns:\n                if df[col].isnull().any():\n                    df[col] = df[col].fillna('Unknown')\n\n        elif name == 'orders.csv':\n            # Expect order_id, client_id, operator_id, order_date, status, price etc.\n            # Drop rows missing order_id or client_id or operator_id\n            critical_cols = ['order_id', 'client_id', 'operator_id']\n            for c in critical_cols:\n                if c in df.columns:\n                    before = df.shape[0]\n                    df = df.dropna(subset=[c])\n                    after = df.shape[0]\n                    if before != after:\n                        print(f\"{name}: Dropped {before - after} rows with missing {c}.\")\n                    # Convert IDs to int\n                    df[c] = pd.to_numeric(df[c], errors='coerce')\n                    before = df.shape[0]\n                    df = df.dropna(subset=[c])\n                    df[c] = df[c].astype(int)\n                    after = df.shape[0]\n                    if before != after:\n                        print(f\"{name}: Dropped {before - after} rows with non-numeric {c}.\")\n            # Parse order_date to datetime if exists\n            if 'order_date' in df.columns:\n                df['order_date'] = pd.to_datetime(df['order_date'], errors='coerce')\n                missing_dates = df['order_date'].isnull().sum()\n                if missing_dates > 0:\n                    print(f\"{name}: Found {missing_dates} invalid order_date entries, dropping those rows.\")\n                    df = df.dropna(subset=['order_date'])\n            # Price column: convert to numeric and fill missing or invalid with 0\n            if 'price' in df.columns:\n                df['price'] = pd.to_numeric(df['price'], errors='coerce').fillna(0)\n                # Replace negative prices with 0 (invalid)\n                neg_prices = (df['price'] < 0).sum()\n                if neg_prices > 0:\n                    print(f\"{name}: Found {neg_prices} negative prices, setting to 0.\")\n                    df.loc[df['price'] < 0, 'price'] = 0\n\n        elif name == 'operators.csv':\n            # Expect operator_id as primary key\n            if 'operator_id' in df.columns:\n                before = df.shape[0]\n                df = df.dropna(subset=['operator_id'])\n                after = df.shape[0]\n                if before != after:\n                    print(f\"{name}: Dropped {before - after} rows with missing operator_id.\")\n                df['operator_id'] = pd.to_numeric(df['operator_id'], errors='coerce')\n                before = df.shape[0]\n                df = df.dropna(subset=['operator_id'])\n                df['operator_id'] = df['operator_id'].astype(int)\n                after = df.shape[0]\n                if before != after:\n                    print(f\"{name}: Dropped {before - after} rows with non-numeric operator_id.\")\n            # Fill missing categorical with 'Unknown'\n            for col in df.select_dtypes(include='object').columns:\n                if df[col].isnull().any():\n                    df[col] = df[col].fillna('Unknown')\n\n        elif name == 'operator_balance.csv':\n            # Expect operator_id, balance, last_update_date\n            if 'operator_id' in df.columns:\n                df['operator_id'] = pd.to_numeric(df['operator_id'], errors='coerce')\n                before = df.shape[0]\n                df = df.dropna(subset=['operator_id'])\n                df['operator_id'] = df['operator_id'].astype(int)\n                after = df.shape[0]\n                if before != after:\n                    print(f\"{name}: Dropped {before - after} rows with invalid operator_id.\")\n            if 'balance' in df.columns:\n                df['balance'] = pd.to_numeric(df['balance'], errors='coerce').fillna(0)\n            if 'last_update_date' in df.columns:\n                df['last_update_date'] = pd.to_datetime(df['last_update_date'], errors='coerce')\n                missing_dates = df['last_update_date'].isnull().sum()\n                if missing_dates > 0:\n                    print(f\"{name}: Found {missing_dates} invalid last_update_date entries, dropping those rows.\")\n                    df = df.dropna(subset=['last_update_date'])\n\n        elif name == 'activity_logs.csv':\n            # Expect activity_id, operator_id, activity_type, timestamp, details\n            if 'activity_id' in df.columns:\n                before = df.shape[0]\n                df = df.dropna(subset=['activity_id'])\n                after = df.shape[0]\n                if before != after:\n                    print(f\"{name}: Dropped {before - after} rows with missing activity_id.\")\n                df['activity_id'] = pd.to_numeric(df['activity_id'], errors='coerce')\n                before = df.shape[0]\n                df = df.dropna(subset=['activity_id'])\n                df['activity_id'] = df['activity_id'].astype(int)\n                after = df.shape[0]\n                if before != after:\n                    print(f\"{name}: Dropped {before - after} rows with non-numeric activity_id.\")\n            if 'operator_id' in df.columns:\n                df['operator_id'] = pd.to_numeric(df['operator_id'], errors='coerce')\n                before = df.shape[0]\n                df = df.dropna(subset=['operator_id'])\n                df['operator_id'] = df['operator_id'].astype(int)\n                after = df.shape[0]\n                if before != after:\n                    print(f\"{name}: Dropped {before - after} rows with invalid operator_id.\")\n            if 'timestamp' in df.columns:\n                df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n                missing_ts = df['timestamp'].isnull().sum()\n                if missing_ts > 0:\n                    print(f\"{name}: Found {missing_ts} invalid timestamp entries, dropping those rows.\")\n                    df = df.dropna(subset=['timestamp'])\n            # Fill missing activity_type with 'Unknown'\n            if 'activity_type' in df.columns:\n                df['activity_type'] = df['activity_type'].fillna('Unknown')\n\n        return df\n\n    # Load all datasets\n    clients = load_csv('clients.csv')\n    orders = load_csv('orders.csv')\n    operators = load_csv('operators.csv')\n    operator_balance = load_csv('operator_balance.csv')\n    activity_logs = load_csv('activity_logs.csv')\n\n    # Show overview before repair\n    for df, name in zip([clients, orders, operators, operator_balance, activity_logs],\n                        ['clients.csv', 'orders.csv', 'operators.csv', 'operator_balance.csv', 'activity_logs.csv']):\n        data_overview(df, name)\n\n    # Repair datasets\n    clients = repair_df(clients, 'clients.csv')\n    orders = repair_df(orders, 'orders.csv')\n    operators = repair_df(operators, 'operators.csv')\n    operator_balance = repair_df(operator_balance, 'operator_balance.csv')\n    activity_logs = repair_df(activity_logs, 'activity_logs.csv')\n\n    # Show overview after repair\n    print(\"\\n=== After Repair & Validation ===\")\n    for df, name in zip([clients, orders, operators, operator_balance, activity_logs],\n                        ['clients.csv', 'orders.csv', 'operators.csv', 'operator_balance.csv', 'activity_logs.csv']):\n        data_overview(df, name)\n\n    # Return repaired dataframes for further analysis if needed\n    return {\n        'clients': clients,\n        'orders': orders,\n        'operators': operators,\n        'operator_balance': operator_balance,\n        'activity_logs': activity_logs\n    }\n```"
    }
]