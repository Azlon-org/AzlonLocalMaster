[
    {
        "role": "user",
        "content": "You are a Python coding assistant. Your task is to generate Python code for the following data analysis task.\nEnsure your code is self-contained, uses common libraries like pandas and numpy, and is ready to run.\nThe code will be executed within a function wrapper, so avoid top-level `if __name__ == '__main__':` blocks in your direct output.\nPrioritize generating actionable business insights, such as identifying trends, patterns, correlations, or anomalies in the data. While basic data cleaning might be necessary if data quality issues impede analysis, the primary goal is to uncover valuable information that can inform business decisions. If the 'Analysis Plan Task to Implement' is specifically about data cleaning or preparation, focus on that. Otherwise, aim for insightful analysis.\nProduce outputs that clearly communicate these insights. This includes printing statistical summaries and key observations to standard output, and generating informative visualizations (e.g., bar charts, line plots, scatter plots, heatmaps as appropriate for the data and task). Save all generated plots to files in the `data_directory_path` (which will be provided as a variable) and print a confirmation message to standard output stating the filename of the saved plot.\n\n**Business Context:**\nQuickWash operates a platform (app and website) that connects car detailers (\u201coperators\u201d) directly with customers. Customers request car wash services through the app, and detailers travel to the customer\u2019s location to perform the wash. The business model revolves around on-demand mobile car washing and detailing services with various service packages and potential extras.\n\n**Data Summary/Available Data:**\nNo data summary found.\n\n**Data Directory Path:**\nmulti_agents/competition/CarWash_Data\n\n**Analysis Plan Task to Implement:**\n**Data Quality and Integrity Assessment**\n\nWhen loading data files (e.g., CSVs), assume a variable `data_directory_path` is predefined in the execution scope and contains the 'Data Directory Path' shown above. Construct full paths to data files using this variable and `os.path.join()`. For example, to load `orders.csv`, use `pd.read_csv(os.path.join(data_directory_path, 'orders.csv'))`.\nPlease include basic error handling, such as try-except blocks around file I/O operations and critical data processing steps.\nPlease provide ONLY the Python code block for this task, without any surrounding explanations unless they are comments within the code."
    },
    {
        "role": "assistant",
        "content": "```python\nimport os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef assess_data_quality_and_integrity(data_directory_path):\n    # List of expected data files - adjust if more files are known\n    expected_files = ['orders.csv', 'operators.csv', 'customers.csv', 'services.csv']\n    loaded_data = {}\n\n    print(\"Starting data quality and integrity assessment...\\n\")\n\n    # Load datasets with error handling\n    for file in expected_files:\n        file_path = os.path.join(data_directory_path, file)\n        try:\n            df = pd.read_csv(file_path)\n            loaded_data[file] = df\n            print(f\"Loaded '{file}' successfully. Shape: {df.shape}\")\n        except FileNotFoundError:\n            print(f\"ERROR: File '{file}' not found in directory '{data_directory_path}'. Skipping.\")\n        except pd.errors.EmptyDataError:\n            print(f\"ERROR: File '{file}' is empty. Skipping.\")\n        except Exception as e:\n            print(f\"ERROR: Could not load '{file}': {e}\")\n\n    print(\"\\n--- Data Quality Reports ---\\n\")\n    for fname, df in loaded_data.items():\n        print(f\"File: {fname}\")\n        print(\"-\" * (6 + len(fname)))\n\n        # Basic info\n        print(\"Basic info:\")\n        buffer = []\n        df.info(buf=buffer)\n        for line in buffer:\n            print(line)\n        \n        # Summary statistics for numeric columns\n        print(\"\\nSummary statistics (numeric columns):\")\n        print(df.describe().T)\n\n        # Missing values\n        missing_counts = df.isnull().sum()\n        missing_percent = 100 * missing_counts / len(df)\n        missing_df = pd.DataFrame({'missing_count': missing_counts, 'missing_percent': missing_percent})\n        missing_df = missing_df[missing_df['missing_count'] > 0].sort_values(by='missing_percent', ascending=False)\n        if not missing_df.empty:\n            print(\"\\nColumns with missing values:\")\n            print(missing_df)\n        else:\n            print(\"\\nNo missing values detected.\")\n\n        # Check for duplicated rows\n        duplicated_count = df.duplicated().sum()\n        print(f\"\\nDuplicated rows: {duplicated_count}\")\n        if duplicated_count > 0:\n            print(\"Consider investigating or removing duplicates.\")\n\n        # Check for possible anomalies in key columns (if known)\n        # For example, dates, IDs, numeric ranges\n        # Here we try to guess some columns by name heuristics\n        for col in df.columns:\n            if 'date' in col.lower() or 'time' in col.lower():\n                # Try parsing datetime, count parsing failures\n                try:\n                    parsed_dates = pd.to_datetime(df[col], errors='coerce')\n                    invalid_dates = parsed_dates.isna().sum()\n                    if invalid_dates > 0:\n                        print(f\"Warning: Column '{col}' has {invalid_dates} invalid/missing date entries.\")\n                except Exception:\n                    pass\n            if col.lower().endswith('id'):\n                # Check uniqueness of IDs\n                unique_ids = df[col].nunique(dropna=True)\n                total = len(df[col].dropna())\n                if unique_ids != total:\n                    print(f\"Note: Column '{col}' has {unique_ids} unique IDs but {total} total non-null entries.\")\n            if df[col].dtype in [np.float64, np.int64]:\n                # Check for outliers using IQR method\n                q1 = df[col].quantile(0.25)\n                q3 = df[col].quantile(0.75)\n                iqr = q3 - q1\n                lower_bound = q1 - 1.5 * iqr\n                upper_bound = q3 + 1.5 * iqr\n                outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n                if not outliers.empty:\n                    print(f\"Column '{col}' has {len(outliers)} potential outliers based on IQR.\")\n\n        print(\"\\n\")\n\n    # Correlation heatmap for numeric data in 'orders.csv' if loaded\n    if 'orders.csv' in loaded_data:\n        orders_df = loaded_data['orders.csv']\n        numeric_cols = orders_df.select_dtypes(include=[np.number]).columns.tolist()\n        if len(numeric_cols) > 1:\n            plt.figure(figsize=(10, 8))\n            corr = orders_df[numeric_cols].corr()\n            sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=\".2f\")\n            plt.title(\"Correlation Heatmap of Numeric Features in orders.csv\")\n            heatmap_path = os.path.join(data_directory_path, 'orders_numeric_correlation_heatmap.png')\n            plt.tight_layout()\n            plt.savefig(heatmap_path)\n            plt.close()\n            print(f\"Correlation heatmap saved to '{heatmap_path}'\")\n        else:\n            print(\"Not enough numeric columns in 'orders.csv' to generate correlation heatmap.\")\n\n    print(\"\\nData quality and integrity assessment completed.\")\n```"
    }
]