[
    {
        "role": "user",
        "content": "You are a Python coding assistant. Your task is to generate Python code for the following data analysis task.\nEnsure your code is self-contained, uses common libraries like pandas and numpy, and is ready to run.\nThe code will be executed within a function wrapper, so avoid top-level `if __name__ == '__main__':` blocks in your direct output.\nPrioritize generating actionable business insights, such as identifying trends, patterns, correlations, or anomalies in the data. While basic data cleaning might be necessary if data quality issues impede analysis, the primary goal is to uncover valuable information that can inform business decisions. If the 'Analysis Plan Task to Implement' is specifically about data cleaning or preparation, focus on that. Otherwise, aim for insightful analysis.\nProduce outputs that clearly communicate these insights. This includes printing statistical summaries and key observations to standard output, and generating informative visualizations (e.g., bar charts, line plots, scatter plots, heatmaps as appropriate for the data and task). Save all generated plots to files in the `data_directory_path` (which will be provided as a variable) and print a confirmation message to standard output stating the filename of the saved plot.\n\n**Business Context:**\nNo business context found.\n\n**Data Summary/Available Data:**\nNo data summary found.\n\n**Data Directory Path:**\nmulti_agents/competition/CarWash_Data\n\n**Analysis Plan Task to Implement:**\n**Data Preparation and Quality Assurance**\n\nWhen loading data files (e.g., CSVs), assume a variable `data_directory_path` is predefined in the execution scope and contains the 'Data Directory Path' shown above. Construct full paths to data files using this variable and `os.path.join()`. For example, to load `orders.csv`, use `pd.read_csv(os.path.join(data_directory_path, 'orders.csv'))`.\nPlease include basic error handling, such as try-except blocks around file I/O operations and critical data processing steps.\nPlease provide ONLY the Python code block for this task, without any surrounding explanations unless they are comments within the code."
    },
    {
        "role": "assistant",
        "content": "```python\nimport os\nimport pandas as pd\nimport numpy as np\n\ndef data_preparation_and_quality_assurance():\n    # List all CSV files in the data directory\n    try:\n        files = [f for f in os.listdir(data_directory_path) if f.endswith('.csv')]\n        if not files:\n            print(\"No CSV files found in the data directory.\")\n            return\n    except Exception as e:\n        print(f\"Error listing files in directory {data_directory_path}: {e}\")\n        return\n\n    # Dictionary to hold dataframes\n    dataframes = {}\n\n    # Load each CSV file with error handling\n    for file in files:\n        file_path = os.path.join(data_directory_path, file)\n        try:\n            df = pd.read_csv(file_path)\n            dataframes[file] = df\n            print(f\"Loaded '{file}' successfully with shape {df.shape}.\")\n        except Exception as e:\n            print(f\"Failed to load '{file}': {e}\")\n\n    # Perform quality checks on each dataframe\n    for file, df in dataframes.items():\n        print(f\"\\n=== Data Quality Report for '{file}' ===\")\n\n        # Check for missing values\n        missing_counts = df.isnull().sum()\n        total_missing = missing_counts.sum()\n        if total_missing == 0:\n            print(\"No missing values found.\")\n        else:\n            print(f\"Missing values detected: {total_missing} total\")\n            print(missing_counts[missing_counts > 0])\n\n        # Check for duplicate rows\n        dup_count = df.duplicated().sum()\n        if dup_count == 0:\n            print(\"No duplicate rows found.\")\n        else:\n            print(f\"Duplicate rows found: {dup_count}\")\n\n        # Basic data type info and conversion suggestions\n        print(\"\\nData types:\")\n        print(df.dtypes)\n\n        # Check numeric columns for outliers using IQR method\n        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n        if numeric_cols:\n            print(\"\\nOutlier detection (IQR method) for numeric columns:\")\n            for col in numeric_cols:\n                Q1 = df[col].quantile(0.25)\n                Q3 = df[col].quantile(0.75)\n                IQR = Q3 - Q1\n                lower_bound = Q1 - 1.5 * IQR\n                upper_bound = Q3 + 1.5 * IQR\n                outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n                outlier_count = outliers.shape[0]\n                print(f\" - {col}: {outlier_count} outliers\")\n\n        # Check for inconsistent categorical data (e.g., leading/trailing spaces, case issues)\n        categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n        if categorical_cols:\n            print(\"\\nCategorical columns unique value counts and potential formatting issues:\")\n            for col in categorical_cols:\n                unique_vals = df[col].unique()\n                # Check for leading/trailing spaces\n                stripped_unique_vals = set([str(val).strip() for val in unique_vals])\n                if len(stripped_unique_vals) < len(unique_vals):\n                    print(f\" - {col}: Possible inconsistent whitespace in categories\")\n                # Check for case inconsistencies\n                lower_unique_vals = set([str(val).lower() for val in unique_vals])\n                if len(lower_unique_vals) < len(unique_vals):\n                    print(f\" - {col}: Possible inconsistent casing in categories\")\n                print(f\"   Unique values count: {len(unique_vals)}\")\n\n    print(\"\\nData preparation and quality assurance completed.\")\n```"
    }
]